Vídeos que infringen sus propias políticas: así traiciona a YouTube su algoritmo, según un informe de Mozilla
######
Una investigación elaborada con miles de voluntarios concluye que el 70% del contenido inapropiado es recomendado por la plataforma
######
“YouTube debe aceptar que su algoritmo está diseñado de una manera que daña y desinforma a las personas”, ha asegurado el responsable senior de Defensa de Mozilla, Brandi Gerkink, tras revelar los hallazgos de una investigación realizada por la Fundación Mozilla —del consorcio desarrollador del navegador Firefox— sobre el funcionamiento de la plataforma de vídeo que alberga más de 2.000 millones de usuarios activos.Las conclusiones son contundentes. Elaborada con datos donados por miles de usuarios de YouTube, la investigación ha revelado que el algoritmo del reproductor de grabaciones recomienda vídeos con información errónea, contenido violento, incitación al odio y estafas, características que la plataforma castiga en sus políticas de uso. La investigación también encontró que las personas en países que no hablan inglés como primer idioma tienen muchas más probabilidades —un 60% más— de encontrar vídeos perturbadores. “Mozilla espera que estos hallazgos, que son solo la punta del iceberg, convenzan al público y a los legisladores de la urgente necesidad de una mayor transparencia en la inteligencia artificial de YouTube”, señala el equipo de investigadores en su informe.“Es difícil para nosotros obtener ninguna conclusión de este informe, ya que nunca definen qué significa ‘rechazables’ y solo comparten unos pocos vídeos, no todo el conjunto de datos”, explica por su parte un portavoz de YouTube, que añade que parte del contenido categorizado como no deseable incluye un tutorial sobre hacer cerámica, un corte del programa de TV Silicon Valley, un vídeo de manualidades y un corte de Fox Business.La investigación tuvo una duración de diez meses y se realizó en conjunto con un equipo de 41 asistentes de investigación empleados por la Universidad de Exeter en Inglaterra. Para obtener los datos revelados, Mozilla utilizó la herramienta RegretsReporter, una extensión de navegador de código abierto que convirtió a miles de usuarios de YouTube en perros guardianes de la plataforma. En otras palabras, los usuarios voluntarios donaron sus datos para que los investigadores tuvieran acceso a un grupo de datos de recomendaciones estrictos de YouTube.Los voluntarios de la investigación encontraron una variedad de “vídeos lamentables”, que informaban de todo. Demasiado quizás. Desde el miedo al coronavirus hasta la desinformación política y caricaturas infantiles “tremendamente inapropiadas”, según la investigación. El 71% de todos los vídeos que los voluntarios definieron como inapropiados o con contenido que hería susceptibilidades fueron recomendados activamente por el propio algoritmo. Así, casi 200 vídeos que YouTube recomendó a los voluntarios ahora se han eliminado de la plataforma, incluidos varios que la compañía consideró que violaban sus propias políticas. Estos vídeos tuvieron un total de 160 millones de visitas antes de ser eliminados y obtuvieron un 70% más de vistas por día que otros vídeos que sí cumplen con las normas de la plataforma.Y hay más. Los vídeos recomendados que violaban las políticas de la plataforma tenían un 40% más de probabilidades de visualización que los vídeos buscados y las recomendaciones no necesariamente estaban relacionadas con el contenido visto. Por ejemplo, en un 43,6% de los casos la recomendación no tenía ninguna relación con los vídeos que el voluntario había consumido. “Nuestros datos públicos muestran que el consumo de contenido cuestionable recomendado es significativamente inferior al 1% y solo entre el 0,16% y el 0,18% de todas las reproducciones en YouTube vienen de contenido que viole las normas”, responde por su parte la compañía.La investigación no solo descubrió numerosos ejemplos de discursos de incitación al odio, de desinformación política y científica y otras categorías de contenido que probablemente infligirían los Lineamientos de la Comunidad de YouTube. También descubrió muchos casos que pintan una imagen más compleja. “Muchos de los vídeos reportados pueden caer en la categoría de lo que YouTube llama ‘contenido límite’, que son vídeos que bordean las fronteras de sus reglas, sin realmente violarlas”, explican desde Mozilla.Fuera del algoritmo, desde YouTube las políticas de comportamiento son muy claras para sus usuarios, o eso parecen. En su blog oficial, la plataforma detalla 11 violaciones a sus políticas. “Te mostramos algunas normas de sentido común que te ayudarán a evitar problemas. Tómalas muy en serio y tenlas presentes siempre. No busques brechas ni tecnicismos para evadirlas”, recomiendan desde YouTube. Las categorías son:En el informe, Mozilla también incluye una serie de recomendaciones para la plataforma. “No solo queremos diagnosticar el problema de recomendación de YouTube, queremos resolverlo. Las leyes de transparencia de sentido común, una mejor supervisión y la presión del consumidor pueden ayudar a mejorar este algoritmo”, explican desde Mozilla. La desarrollada de Firefox propone a YouTube publicar informes de transparencia frecuentes y completos que incluyan información sobre sus algoritmos de recomendación así como brindar a las personas la opción de optar por no recibir recomendaciones personalizadas y promulgar leyes que exijan la transparencia del sistema de inteligencia artificial y protejan a los investigadores independientes.“Durante años, YouTube ha recomendado desinformación sobre salud, desinformación política, discurso de odio y otro contenido lamentable. Desafortunadamente, la plataforma se ha enfrentado a las críticas con inercia y opacidad mientras las recomendaciones dañinas persisten”, concluye la investigación.
######
Juan Diego Godoy
######
17 jul 2021-03:20 UTC