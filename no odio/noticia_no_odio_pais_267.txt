Apple revisar√° todas las fotos que se hagan desde sus dispositivos en busca de contenidos pederastas
######
La tecnol√≥gica sigue los pasos de Google, que ya escaneaba de forma autom√°tica las im√°genes que se suben a la nube y pone en alerta a los activistas de la privacidad digital
######
Apple se sube al carro de la vigilancia de contenidos. La compa√±√≠a de Cupertino ha anunciado que a finales de este a√±o pondr√° en marcha en Estados Unidos un sistema que revisar√° autom√°ticamente las im√°genes subidas desde iPhones e iPads en busca de contenidos pederastas (CSAM, child sexual abuse material).Se suma as√≠ a lo que ya ven√≠a haciendo Google, que tambi√©n escanea las fotos y v√≠deos que se suben en su nube (Google Drive) para detectar ese tipo de archivos.El anuncio, avanzado ayer por el Financial Times, se produce tan solo un mes despu√©s de que el Parlamento de la Uni√≥n Europea aprobara suspender temporalmente la Directiva de la privacidad online precisamente para que las operadoras tengan el paraguas legal necesario para combatir la difusi√≥n de pornograf√≠a infantil. En Estados Unidos, el Centro Nacional para Menores Desaparecidos y Explotados (National Center for Missing and Exploited Children, NCMEC) hace tiempo que solicita la cooperaci√≥n de las tecnol√≥gicas para perseguir a los ped√≥filos.La decisi√≥n de Apple ha sentado como un jarro de agua fr√≠a a los activistas en defensa de la privacidad digital. El temor de fondo es que, una vez abierta la puerta a la revisi√≥n de contenidos privados de los tel√©fonos para detectar a potenciales criminales, se pueda extender ese escrutinio a otros objetivos bajo argumentos variopintos.üö®üö® Apple says to "protect children," they're updating every iPhone to continuously compare your photos and cloud storage against a secret blacklist. If it finds a hit, they call the cops.iOS will also tell your parents if you view a nude in iMessage.https://t.co/VZCTsrVnncApple se ha esforzado en explicar que el m√©todo que ha dise√±ado para vigilar los contenidos interfiere poco en la privacidad de los usuarios. El procedimiento es el siguiente: cada vez que una foto tomada desde un dispositivo Apple se sube a la nube (iCloud), se cruza autom√°ticamente su hash (una especie de matr√≠cula de los archivos comprimidos) con el de una lista de contenidos ya identificados como ped√≥filos. Si salta una coincidencia, se lanza un aviso a Apple y un empleado revisar√° los archivos para comprobar si son o no censurables. En caso afirmativo, se suspende la cuenta de Apple del usuario y se avisa a las autoridades.La compa√±√≠a sostiene que, mediante este sistema, nunca pone las manos sobre los archivos privados de los usuarios: el proceso se realiza dentro del dispositivo y solo alerta en caso de alarma.Paralelamente a esta medida, la empresa de la manzana monitorizar√° con un sistema de machine learning, o aprendizaje autom√°tico, las comunicaciones realizadas por menores en su aplicaci√≥n Messages, poco usada en Europa pero muy popular en Estados Unidos. Si se detecta que se est√°n enviando o recibiendo im√°genes con contenidos expl√≠citos se alertar√° a los padres.Apple siempre ha hecho bandera de su pol√≠tica de respeto a la privacidad de sus clientes, que esgrime como un importante elemento de diferenciaci√≥n respecto a su competidor Google. El CEO de la compa√±√≠a, Tim Cook, se enfrent√≥ en 2016 al Gobierno de EE UU al negarse ‚Äúa construir una puerta trasera para el iPhone‚Äù a petici√≥n del FBI, como declar√≥ en 2016. La historia se repiti√≥ en 2020, cuando la tecnol√≥gica se neg√≥ a abrir el iPhone de un sospechoso de terrorismo con los mismos argumentos.La consistencia de esta visi√≥n de la privacidad se hizo patente cuando hace unos meses introdujo un cambio importante en el sistema operativo iOS: desde abril, las aplicaciones que quieran rastrear al usuario deber√°n pedirle permiso; si no se les da, no obtendr√°n datos sobre su actividad digital. Fue un giro muy aplaudido por los defensores de la privacidad y se interpret√≥ como un elemento m√°s de diferenciaci√≥n con sus rivales Google y Facebook, cuyas pol√≠ticas de privacidad no incluyen esa opci√≥n.I‚Äôve had independent confirmation from multiple people that Apple is releasing a client-side tool for CSAM scanning tomorrow. This is a really bad idea.Por eso ha sorprendido tanto la medida reci√©n anunciada. India McKinney y Erica Portnoy, del influyente think tank estadounidense Electronic Frontier Foundation, sostienen en un comunicado que ‚Äúla explotaci√≥n sexual de menores es un problema muy serio, y Apple no es la primera tecnol√≥gica que altera su pol√≠tica de privacidad para tratar de combatirlo. Pero esa decisi√≥n conlleva un alto precio para la privacidad de los usuarios‚Äù. Las investigadoras no dudan de las buenas intenciones de Apple, ni de que la empresa haya hecho lo posible para salvaguardar la privacidad de sus clientes, pero ‚Äúincluso una puerta trasera cuidadosamente estudiada y muy afinada no deja de ser una puerta trasera‚Äù.Por su parte, Diego Naranjo, asesor pol√≠tico de EDRI, una ONG europea que trabaja por la defensa de los derechos humanos en la era digital, opina que ‚ÄúApple ha decidido socavar la encriptaci√≥n de extremo a extremo y hacer que sus usuarios sean vulnerables a la censura y la vigilancia‚Äù. ‚ÄúAl permitir el escaneado de fotos en las comunicaciones privadas y en iCloud, los productos de Apple se convertir√°n en una amenaza para sus due√±os. La compa√±√≠a deber√≠a retractarse‚Äù.
######
M. G. Pascual
######
06 ago 2021-14:00 UTC